{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "id": "b6c56c940ab55394"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "array = np.arange(24)\n",
    "array2 = array.reshape(3,2,4)\n",
    "print(array2)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#print(array2[[1],:,1:3])\n",
    "#print(array2[[2],[1],:])\n",
    "print(array2.min(axis=0))"
   ],
   "id": "d000d14231465cc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "ad3ffbcaffb8cf7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cars = pd.read_csv('cars.csv')\n",
    "#cars.describe()\n",
    "#cars.info()\n",
    "cars.head(5)"
   ],
   "id": "a813afc41b7f6b77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "afe9dd6136144131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cars[['brand','owner']].head(5)",
   "id": "ec858bf75ed0ebb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_cars(city,owner):\n",
    "    mask1 = cars['brand']==city\n",
    "    mask2 = cars['owner']==owner\n",
    "    return cars[mask1 & mask2].shape[0]\n",
    "print(get_cars(\"Skoda\",\"First Owner\"))"
   ],
   "id": "2c9ba747399042c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(cars['owner'].value_counts())\n",
    "t_car = cars[cars['owner']==\"Fourth & Above Owner\"]\n",
    "print(t_car['brand'].value_counts())"
   ],
   "id": "84dbd37355e200ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cars['brand'].value_counts().plot(kind='bar',figsize=(10,5))"
   ],
   "id": "645e7e7c3e2aaf28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "brand = cars.groupby('brand')",
   "id": "27e5354e373e7179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "brand.size().head(1)",
   "id": "25f9b186e86f4901",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "brand['km_driven'].mean().head(1)",
   "id": "60cc32db462ec630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "brand.get_group('BMW').head(1)",
   "id": "797e4a52862cfbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cars.pivot_table(index='brand',columns = 'owner',values='km_driven',aggfunc='mean')\n",
    "cars.head(1)"
   ],
   "id": "7423eebd5b6c196a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(cars['selling_price'])"
   ],
   "id": "5cc5ac772705caf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##sns.boxplot(cars['selling_price'])\n",
    "cars[cars['selling_price']==cars['selling_price'].max()]"
   ],
   "id": "ffbf8553e831ca68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "titanic = pd.read_csv('titanic_toy.csv')\n",
    "titanic.head()"
   ],
   "id": "3d6572fb7dfa3388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(titanic.drop(('Survived'),axis=1),titanic['Survived'], test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n"
   ],
   "id": "54976c853d86e63b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ],
   "id": "8831e04420f11272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_test_scaled = pd.DataFrame(x_test_scaled,columns=x_test.columns)\n",
    "x_test_scaled.head(10)"
   ],
   "id": "b228c0fd2541ef48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "titanic.head()",
   "id": "852b37e32df0428d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming Titanic dataset is loaded in a DataFrame called 'titanic'\n",
    "sns.histplot(titanic['Age'], kde=True)  # For Age\n",
    "plt.title('Age Distribution')\n",
    "plt.show()\n"
   ],
   "id": "c4dfe7650f465828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FEATURE SCALING\n",
    "- KNN, SVM, logistic regression, neural networks -> distance based -> need feature scaling\n",
    "- Tree based models -> not sensitive to feature scaling\n",
    "\n",
    "#### Min Max scaling\n",
    "1. x' = x - x min / (x max - x min)\n",
    "2. Best when we need all feature to have equal weight\n",
    "3. use when KNN, neural networks or SVM\n",
    "\n",
    "#### Z score normalisation (Standardizaton)\n",
    "1. x' = x - mean / Ïƒ\n",
    "2. suitable for algorithm needing normally distributed features\n",
    "3. use when regression models, SVM or PCA\n",
    "\n",
    "#### Robust Scaling\n",
    "1. good against outliers, use when algorithm sensitive to outliers\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "864db9df797a9843"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "numerical_cols = ['battery', 'rating', 'reviews', 'price']\n",
    "scaler = MinMaxScaler()\n",
    "mobile[numerical_cols] = scaler.fit_transform(mobile[numerical_cols])\n",
    "print(mobile.head())\n",
    "\n",
    "# For z score normalisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "mobiles[numerical_cols] = scaler.fit_transform(mobiles[numerical_cols])\n",
    "print(mobiles.head())"
   ],
   "id": "7df1ad17e582fbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ENCODING CATEGORICAL DATA\n",
    "- nominal encoding -> this is when there's no relation between categories\n",
    "- ordinal encoding -> when relationship between categories e.x. excellent, good\n",
    "- label encoding -> ordinal encoding but specifically made for output\n",
    "\n",
    "### ORDINAL ENCODING\n",
    "- if we dont want to run some columns then we can slice the data and then merge later\n",
    "- e.x. final_data = pd.concat([encoded_df, data['col3']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b5a171ab121454b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "oe = OrdinalEncoder(categories=[['Poor','Average','Good'],['School','UG','PG']])\n",
    "x_train = oe.fit_transform(x_train)\n",
    "x_test = oe.transform(x_test)\n",
    "le = LabelEncoder()\n",
    "# fit on y_train and y_test this does not need any input"
   ],
   "id": "87a0901c770dd1df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ONE HOT ENCODING\n",
    "- pandas one hot encoding\n",
    "- scikit learn one hot encoding\n",
    "- k-1 one hot encoding\n",
    "- one hot encoding with top categories -> when we too many categories\n",
    "\n",
    "#### drop = 'first'\n",
    "- this is done to avoid redundancy\n",
    "- blue, green, red ball encoding problem\n",
    "- avoid this for tree based models like xgboost, random forest, decision forest etc\n",
    "\n"
   ],
   "id": "fa8db1dd0b76f873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "x_train_new = ohe.fit_transform(x_train[['fuel','owner']])\n",
    "x_test_new = ohe.transform(x_test[['fuel','owner']])"
   ],
   "id": "f67253c611ea26d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## COLUMN TRANSFORMER\n",
    "- tnf1 etc are just name give to find the columns if we want to see the pipelines\n",
    "\n"
   ],
   "id": "9f01dc75949bac3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer # to fill in the missing values\n",
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('tnf1',SimpleImputer(),['fever']),\n",
    "    ('tnf2',OrdinalEncoder(categories=[['Mild','Strong']]),['cough']),\n",
    "    ('tnf3',OneHotEncoder(sparse=False,drop='first'),['gender','city'])\n",
    "],remainder='passthrough')\n",
    "transformer.fit_transform(x_train)\n",
    "transformer.transform(x_test)"
   ],
   "id": "512cb157ace506b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PIPELINES\n",
    "- multiple steps chained together\n",
    "- missing values impute -> encoding -> scaling -> feature selection -> decision tree model\n",
    "- multiple columns transformers -> finally added to one pipeline\n",
    "#### Pipeline VS make_pipeline\n",
    "- pipelines requires naming of steps, make_pipeline does not"
   ],
   "id": "8c588fdf6f09a92f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pipe = Pipeline([\n",
    "    ('trf1',trf1),\n",
    "    ('trf2',trf2),\n",
    "    ('trf3',trf3),\n",
    "    ('trf4',trf4),\n",
    "    ('trf5',trf5)\n",
    "])\n",
    "# where trf1 etc. are all column transformers, feature selection, model training\n",
    "y_pred = pipe.predict(x_test)"
   ],
   "id": "8e83584ec33e3301"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## FUNCTION TRANSFORMERS\n",
    "- implemented on columns to make data normal -> algorithm performance better\n",
    "- log transform -> cannot be applied on -ve values\n",
    "- reciprocal\n",
    "- power\n",
    "### POWER TRANSFORMERS\n",
    "- Box-cox -> only +ve\n",
    "- y(lambda) =\n",
    "\\begin{cases}\n",
    "\\frac{y^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\ln(y), & \\text{if } \\lambda = 0\n",
    "\\end{cases}\n",
    "\n",
    "- yeojohsnon -> modification of boxcox, applied to -ve also\n",
    "- y(lambda) = \\begin{cases}\n",
    "\\frac{(y + 1)^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\text{ and } y \\geq 0 \\\\\n",
    "\\ln(y + 1), & \\text{if } \\lambda = 0 \\text{ and } y \\geq 0 \\\\\n",
    "\\frac{-( (-y + 1)^{2 - \\lambda} - 1 )}{2 - \\lambda}, & \\text{if } \\lambda \\neq 2 \\text{ and } y < 0 \\\\\n",
    "-\\ln(-y + 1), & \\text{if } \\lambda = 2 \\text{ and } y < 0\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "#### CHECK DATA\n",
    "- use QQ plot\n",
    "- if normally distributed, all points on the line of QQ plot\n",
    "- if skewed right -> line going above ideal at end\n",
    "- if skewed left -> line going below ideal at beginning"
   ],
   "id": "c531caa39dc684cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "trf=FunctionTransformer(np.log1p) # logtransform we just log the values of column\n",
    "# fit_transform this on x_train and transform x_test\n",
    "# np.log1p adds 1 to value and then logs it, hence doesn't gives error if value - 0\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer() # by default works on yeo johnson just need to fit and transform"
   ],
   "id": "c9d4d61815c337d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BINNING\n",
    "- grouping values in range\n",
    "- helps in handling outliers\n",
    "## UNSUPERVISED BINNING\n",
    "### EQUAL WIDTH\n",
    "- no change in the spread of data\n",
    "- handles outliers\n",
    "\n",
    "### EQUAL FREQUENCY\n",
    "- handle outliers\n",
    "- make data more uniform\n",
    "\n",
    "### K BINNING"
   ],
   "id": "36591f7c3c1f167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "kbin_age = KBinsDiscretizer(n_bins=15,encode='ordinal',strategy='quantile')\n",
    "# add some other binner on other column and make a columntransformer"
   ],
   "id": "8ecc8720260aa135"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- see how to deal with mixed data, that is when data contains letter and number e.x. B53 PRANJWAL4798 etc\n",
    "- see how to work with dates, python can convert columns to date type. written in notebook\n",
    "\n",
    "## HANDLING MISSING DATA\n",
    "### 1) REMOVING THE ROWS\n",
    "- when data missing at random\n",
    "- data missing <5% of actual data\n",
    "### 2) IMPUTING THEM (FILLING)\n",
    "- univariate(numerical, categorical)\n",
    "- multivariate(KNN, iterative imputer)\n",
    "\n",
    "#### Handling missing numerical data\n",
    "- normal distribution -> mean/median\n",
    "- skewed dis -> median\n",
    "- randomly  -> useful with linear regression not with decision tree\n",
    "\n",
    "#### Handling categorical missing data\n",
    "- most frequent\n",
    "- new missing\n",
    "- random"
   ],
   "id": "b23c2360c46a2b2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df.isnull().mean()*100\n",
    "from sklearn.impute import SimpleImputer\n",
    "sip = SimpleImputer(strategy='median') # default is mean\n",
    "# most_frequent etc\n",
    "\n",
    "#categorical ->\n",
    "df['categorical_col'].fillname('TA', inplace=True)\n",
    "#or use a simple imputer with strategy = most_frequent\n",
    "from sklearn.impute import SimpleImputer\n",
    "sip = SimpleImputer(strategy='most_frequent')\n",
    "transformed_mobiles = sip.fit_transform(mobiles)\n",
    "#this changes to numpy array -> change back to dataframe\n",
    "transformed_mobiles = pd.DataFrame(transformed_mobiles, columns=mobiles.columns)"
   ],
   "id": "ebadfe11e966243b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNNImputer\n",
    "- more accurate, but has more calculations\n",
    "- missing values imputed from mean from nearest neighbour (distance) found in training set"
   ],
   "id": "c33708aed74541de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T13:00:12.507024Z",
     "start_time": "2025-01-08T13:00:12.387420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn = KNNImputer(n_neighbors=3)"
   ],
   "id": "3f2577310e0e751d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MICE(multivariate imputation by chained equations)\n",
    "- use when missing at random\n",
    "- do not use when not missing at random\n",
    "- iterative imputation\n",
    "- check obsidian note"
   ],
   "id": "77dcd01a140781bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T18:14:14.427811Z",
     "start_time": "2024-12-25T18:14:14.420663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from sklearn.impute import IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "#for mice"
   ],
   "id": "c4fc41026d7ce6c2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OUTLIER DETECTION\n",
    "- trimming (removing)\n",
    "- capping (limiting)\n",
    "- assuming them as missing values\n",
    "- discretization\n",
    "\n",
    "### For detecting outliers\n",
    "- z score treatment -> works on col with almost normal distribution; z_score>3 -> outlier\n",
    "- IQR based filtering -> boxplot Q1-1.5 * IQR , Q3+1.5 * IQR\n",
    "- percentile -> 1 to 99 percentile boundary\n",
    "- wiscorization\n",
    "\n",
    "\n",
    "##### DETECTION\n",
    "- use boxplot for detection\n",
    "- make a function to set outliers to min,max according to their groups\n"
   ],
   "id": "6614b7ea32ee877e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#we can do capping using the np.where statement where we pass 3 parameters (condition,if true,if false)\n",
    "# capping ->\n",
    "\n",
    "df['cgpa'] = np.where(np['cgpa']>max_cgpa, max_cgpa,np.where(df['cgpa']<min_cgpa,min_cgpa,df['cgpa']))\n",
    "gby = data.groupby('Status')\n",
    "def cap_outliers_iqr(group):\n",
    "    q1 = group[\"Time passed\"].quantile(0.25)\n",
    "    q3 = group[\"Time passed\"].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    group[\"Time passed\"] = np.where(group[\"Time passed\"] > upper, upper,\n",
    "                                    np.where(group[\"Time passed\"] < lower, lower, group[\"Time passed\"]))\n",
    "    return group\n",
    "\n",
    "transformed= gby.apply(cap_outliers_iqr)\n",
    "#Or simply pass the dataset if you don't want to set cap by groups\n",
    "transformed_mobile = cap_outliers_iqr(mobiles)\n",
    "\n",
    "# in this case Status had 5 groups namely\n",
    "# Approved - License Issued     24.266246\n",
    "# Denied                         9.541679\n",
    "# Incomplete                   363.039448\n",
    "# Pending Fitness Interview     34.183646\n",
    "# Under Review                  27.704749\n",
    "# printing std deviation after each group.\n",
    "# if using a boxplot better to use quantile than std and mean\n",
    "# CHECK USING BOXPLOT\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x=transformed[\"Time passed\"],y=transformed[\"Status\"],color=\"skyblue\")\n",
    "# x axis is the time passed (numerical column) and y axis is categorical column (status)\n",
    "\n",
    "\n",
    "#if greater than max_limit -> set to max_limit\n",
    "#otherwise set to min_limit and if not keep as it is (for normal values)"
   ],
   "id": "be0961b9de20e53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## FEATURE CONSTRUCTION\n",
    "### CURSE OF DIMENTIONALITY\n",
    "- higher dimension -> dangerous for algorithms\n",
    "- too many features (columns) -> efficiency decreases\n",
    "\n",
    "#### Feature Selection\n",
    "- forward selection\n",
    "- backward elimination\n",
    "\n",
    "#### Feature Extraction\n",
    "- PCA\n",
    "- LDA\n",
    "- t-sne"
   ],
   "id": "cc3a24fcd07e30d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PCA (Principle Component Analysis)\n",
    "- rotates the axis (if linear relationship) since that'll make a straight line graph\n",
    "- STEPS :\n",
    "- centarization of data (mean = 0)\n",
    "- scale the data optionally\n",
    "- calculate covariance matrix -> COV(x) = 1/n * X^T * X\n",
    "- compute eigen values (amount of variance explained by each principal comp) and eigen vectors (represent the direction (axis) for them)\n",
    "- Z = X * W\n",
    "- where X = original matrix, W = matrix of eigen vec, Z = transformed data\n",
    "\n",
    "#### PCA doesn't work where\n",
    "- data has specific pattern\n",
    "- special form of data (parabola, seperated in halves, circle)\n"
   ],
   "id": "5fffaeeedd6b2a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1 - Apply standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df.iloc[:,0:3] = scaler.fit_transform(df.iloc[:,0:3])\n",
    "\n",
    "# Step 2 - Find Covariance Matrix\n",
    "covariance_matrix = np.cov([df.iloc[:,0],df.iloc[:,1],df.iloc[:,2]])\n",
    "print('Covariance Matrix:\\n', covariance_matrix)\n",
    "# Step 3 - Finding EV and EVs\n",
    "eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "transformed_df = np.dot(df.iloc[:,0:3],pc.T)\n",
    "# 40,3 - 3,2\n",
    "new_df = pd.DataFrame(transformed_df,columns=['PC1','PC2'])\n",
    "new_df['target'] = df['target'].values\n",
    "new_df.head()\n",
    "\n",
    "# check github file"
   ],
   "id": "31ee445553ba1064"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SIMPLE LINEAR REGRESSION\n",
    "- y=mx+b\n",
    "#### Closed Form Sol\n",
    "- Ols (ordinary least square) -> direct formula\n",
    "- 'm' (slope) comes from differentiating the error function and equating it to zero (hence minimizing it).\n",
    "1. error function (loss function) is the cumulative sum of square of distance of all points from best fit line.\n",
    "\n",
    "\n",
    "#### Non Closed Form Sol\n",
    "- gradient descent (for higher dimension)\n",
    "\n",
    "\n",
    "#### REGRESSION METRICS\n",
    "- MAE (mean absolute)\n",
    "1. robust to outliers\n",
    "- MSE (mean squared)\n",
    "\n",
    "1. not robust to outliers\n",
    "\n",
    "- RMSE (root mean squared)\n",
    "1. not robust to outliers\n",
    "\n",
    "- R2\n",
    "1. how much linear reg better than mean line\n",
    "2. closed to 1 -> better formula -> (1 - [error (reg) / error (mean)])\n",
    "3. R2 -> 0.8 -> can explain 80% of data\n",
    "\n",
    "- Adjusted R2\n",
    "1. adding more cols -> increases R2 error, hence unnecessary cols can increase R2 score\n",
    "2. adjusted R2 removes this\n",
    "\n"
   ],
   "id": "dff98b96c7438444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T11:58:36.924715Z",
     "start_time": "2025-01-03T11:58:36.922569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "# for r2 -> have to make it ourselve"
   ],
   "id": "351769f119c64ae8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MULTIPLE LINEAR REGRESSION\n",
    "e.x output = bias/intercept + b1*col1 + b2*col2\n",
    "(b1,b2 -> weights)\n",
    "training same as normal lr\n",
    "OLS -> matrix inversion -> guass jordan (time complexity O(n^3)\n"
   ],
   "id": "2dd1f452569446dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c0a670edbebf1a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GRADIENT DESCENT\n",
    "- minimising error b/w predicted and actual y\n",
    "- batch GD\n",
    "1. slower, needs more epochs (gos)\n",
    "2. e.x. n = 10^5, col = 10^2 , epoch = 10^3 -> 10^10 computations, cannot take large datasets\n",
    "\n",
    "- SGD\n",
    "1. less accurate, updates after every line\n",
    "2. takes less epoch (5-10)\n",
    "3. if same epoch as batch GD -> total iteration = epoch * n (no. of rows) hence slower\n",
    "4. handles big data\n",
    "\n",
    "- MBGD (mini batch gradient descent)\n",
    "1. cross between batch and stochastic.\n",
    "\n",
    "- have to take care of local minima since we have to look for minimum of the parabola)\n",
    "- minimising loss function\n",
    "\n",
    "1. b new = b old - slope*n\n",
    "2. Î¸:=Î¸âˆ’Î±âˆ‡J(Î¸) where Î± -> learning rate and âˆ‡J(Î¸) -> gradient of the loss function\n",
    "\n",
    "#### Learning Rate\n",
    "- too small -> slow, can get struck in region w/ very flat gradient\n",
    "- too large -> will overshoot the minimum\n",
    "- optimal -> start small (0.01) then adjust it"
   ],
   "id": "d639d1a54230bb7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(max_iter=1000) # etc"
   ],
   "id": "dae9e1590415d83d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Regularization -> set of methods used for reducing overfitting in ML models\n",
    "\n",
    "### Attributes in SGDRegressor\n",
    "1. max_iter -> epochs, how many times GD runs over dataset, increase if struggle to converge\n",
    "2. tol -> for stopping criteria, default 10^-3, if improvement loss function less than tol, training stops\n",
    "3. loss -> default (squared_error), use huber if outlier present\n",
    "4. learning_rate -> 'optimal' or 'adaptive' recommended\n",
    "5. eta0 -> initial learning rate when *learning rate chosen as optimal, adaptive, invscaling* default is 0.01\n",
    "6. penalty -> l1, l2 or elastinet\n",
    "7. alpha -> regularization strength, default 10^-4, higher alpha helps prevent overfitting but can cause underfit\n",
    "8. l1_ratio -> (use when penalty = 'elastinet') range -> [0,1], 0 -> pure L2 and 1 -> pure L1.\n"
   ],
   "id": "3fcdc3ee3d6a6279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd71a492e0782018"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
