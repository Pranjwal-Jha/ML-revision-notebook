{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## REMOVING URLS\n",
    "- use regex\n",
    "## REMOVE PUNCTUATIONS\n",
    "-"
   ],
   "id": "7bb8abb82fb8329e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "import spellchecker\n",
    "\n",
    "exclude = string.punctuation + string.digits + '?'\n",
    "print(exclude)"
   ],
   "id": "3fd54049aa98e895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_punctuation(text):\n",
    "    for char in exclude:  # iterating through every element in exclude\n",
    "        text = text.replace(char, '') #if we find that in text, replace it with ' '\n",
    "    return text\n"
   ],
   "id": "8f1ae0a514f2eb32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = 'Hello my name is pranjwal ! jha > ? !'\n",
    "print(remove_punctuation(text))"
   ],
   "id": "c8da5c1d724d1c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_punc(text):\n",
    "    text = text.translate(str.maketrans('','',exclude))\n",
    "    return text"
   ],
   "id": "4bbd6f9393c4d680",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"Hello my name is pranjwal ! jha > ? !\"\n",
    "print(remove_punc(text))"
   ],
   "id": "266c7b02f483a9e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "118ae401936d5215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spelling errors\n",
    "- using correction libraries like textblob"
   ],
   "id": "841d7673f24c2df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "id": "9bd10308362e34d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Removing Stop words\n",
    "- using prebuilt library"
   ],
   "id": "f9b12a3156d1207b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ],
   "id": "d6b524f9d4e1f345",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            # text = text.replace(word, '')\n",
    "    return ' '.join(new_text)   # converting list to a string"
   ],
   "id": "2bd447637d77c83a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(remove_stopwords(\"why am i here ? pranjwal jha\"))",
   "id": "874a06aa88f0c685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TOKENIZATION\n",
    "- better than her regular expression\n",
    "- nltk.tokenize gives decent result\n",
    "- best result with SPaCy -> library\n",
    "\n",
    "\n",
    "## SpaCy\n",
    "- make sentence document before processing first\n"
   ],
   "id": "ab20291b407ed6d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Define a sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "# Print out the sentence's tokenized words, parts of speech, and dependency parse tree\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}, Dep.: {token.dep_}\")\n",
    "# Extract named entities (e.g., people, organizations)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")\n",
    "# Get the sentence's dependency parse tree\n",
    "print(\"Dependency Parse Tree:\")\n",
    "for token in doc:\n",
    "    if token.head != token and token.dep_ not in ['ROOT', 'punct']:\n",
    "        print(f\"{token.text} -> {token.head.text} ({token.dep_})\")"
   ],
   "id": "4e61246e908d7439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## STEMMING\n",
    "- Changing words into their base forms -> into root word\n",
    "- even if a root word (stem) is not a valid word in the language\n"
   ],
   "id": "9111a5a0cc03f2e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    stem_words = [ps.stem(word) for word in words]  # using stemming for each word\n",
    "    return \" \".join(stem_words) # converting list into a string\n",
    "    # return stem_words"
   ],
   "id": "2290ea2e356002e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = \"walk walks walked walking\"\n",
    "print(stem_sentence(a))"
   ],
   "id": "240bb2c1ff9a96b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LEMMATIZATION\n",
    "- always a english word\n",
    "- slow compared to stemming since it looks for an actual english word\n"
   ],
   "id": "b5a57c72387fdc21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TEXT REPRESENTATION\n",
    "## BAG OF WORDS\n",
    "1. Uses a matrix to store the words which have occured in a particular sentence -> plots them on a graph\n",
    "2. lesser then angle between words in graph -> more similar they are"
   ],
   "id": "12941a699451a324"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I love learning new things\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Python programming is fun\"\n",
    "]"
   ],
   "id": "d382771fc9760154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform it into a sparse matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (unique words)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to dense array for better understanding\n",
    "dense_matrix = X.toarray()\n",
    "\n",
    "# Display results\n",
    "print(\"Feature Names:\")\n",
    "print(features)\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "print(dense_matrix)"
   ],
   "id": "5d54a5e6dee5588c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## N GRAMS\n",
    "1. splitting into 2 or more combination of words rather than a sngle word\n",
    "2. Bag of words is special case of N GRAMS with split happening for each word (uni-gram)\n",
    "3. able to capture semantic of the sentence\n",
    "\n",
    "#### DISADVANTAGES OF N GRAMS\n",
    "1. dimension increased -> slows down the algorithm\n",
    "2. no solution for OOV (other than to ignore)"
   ],
   "id": "284b07deac084db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trigram = CountVectorizer(ngram_range=(1,3)) # makes uni + bi + tri gram splits",
   "id": "feadf2f234458887",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TF - IDF\n",
    "1. term frequency - inverse document frequency to measure how important a word is in a document compared to in a collection of documents (corpus)\n",
    "2. TF-IDF = TF*IDF (product)\n",
    "\n",
    "#### DISADVANTAGE\n",
    "1. sparsity\n",
    "2. OOV\n",
    "3. dimension problem (too big of a dimension if vocabulary too big)\n",
    "4. cannot capture semantic relationship"
   ],
   "id": "c8731f3166c2231b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()"
   ],
   "id": "1e67811edd0f5719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## WORD2VEC\n",
    "1. can capture the semantic meaning\n",
    "2. dense vector\n",
    "3. low dimension vector made (not reaching large dimension like bow & tf-idf)"
   ],
   "id": "c3b7729d68b18555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5700d7f01210bae8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
